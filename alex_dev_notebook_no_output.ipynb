{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project - Facial Keypoint Recognition \n",
    "#### Alex Carite | Oscar Linares | Greg Rosen | Shehzad Shahbuddin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "import time\n",
    "import os.path\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, GlobalAveragePooling2D, LeakyReLU\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7, EfficientNetB0\n",
    "\n",
    "np.random.seed(0)\n",
    "print (\"OK\")\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see if the train/test csv are loaded, if not, unzip from dir\n",
    "if (os.path.exists('training.csv') == False):\n",
    "    !unzip training.zip\n",
    "else:\n",
    "    print('training data already unzipped')\n",
    "\n",
    "if (os.path.exists('test.csv') == False):\n",
    "    !unzip test.zip\n",
    "else:\n",
    "    print('test data already unzipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('training.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to clean up a few NA's in our set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for data shapes\n",
    "def two_dim(image):\n",
    "    'takes in an image vector of 9,216 pixels and makes it into a 96x96 shape'\n",
    "    return np.array(image.split(' '), dtype=int).reshape(96, 96)\n",
    "\n",
    "def make_array(image):\n",
    "    return np.array(image.split(' '), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform all data\n",
    "X = np.array([two_dim(train.Image[i]) for i in range(len(train))])\n",
    "# X_tmp = X_tmp / 255.0\n",
    "# X = np.array([make_array(train.Image[i]) for i in range(len(train))])\n",
    "X = X / 255.0\n",
    "Y = np.array([train.drop('Image', axis = 1).iloc[i] for i in range(len(train))])\n",
    "\n",
    "#Need to make X_test and Y_test (will we have a Y_test? I think that may be handled in Kaggle)\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "train_data, train_labels = X[:5000], Y[:5000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]\n",
    "dev_data, dev_labels = X[5000:], Y[5000:]\n",
    "numFeatures = train_data[1].size\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numMiniExamples = mini_train_data.shape[0]\n",
    "numDevExamples = dev_data.shape[0]\n",
    "# numTestExamples = test_data.shape[0]\n",
    "print(f'Train examples {numTrainExamples}')\n",
    "print(f'Train features {numFeatures}')\n",
    "print(f'mini_Train examples {numMiniExamples}')\n",
    "print(f'Dev examples {numDevExamples}')\n",
    "# print(f'Test examples {numTestExamples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view missing values\n",
    "import missingno as msno\n",
    "fig, ax = plt.subplots()\n",
    "msno.bar(train)\n",
    "ax.set_title(\"Missing Values\", fontsize = 30, pad = 20)\n",
    "ax.set_xlabel(\"Feature\", fontsize = 25, labelpad = 20)\n",
    "ax.set_ylabel(\"Fill Rate\", fontsize = 25, labelpad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA see how many na's for each column in the training dataset\n",
    "train.isna().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Removing Missing Values)\n",
    "We lose 70% of the dataset when removing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates \n",
    "train.duplicated(subset = ['Image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all NAs to have a clean fully labeled dataset\n",
    "train_noNA = train.dropna(axis=0, how='any', inplace = False)\n",
    "train_noNA = train_noNA.reset_index(drop=True)\n",
    "print(len(train))\n",
    "print(len(train_noNA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unlab = train[train.isnull().any(1)]\n",
    "train_unlab = train_unlab.reset_index(drop = True)\n",
    "print(len(train))\n",
    "print(len(train_unlab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create arrays for no-NA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform No NaN data\n",
    "X_noNA = np.array([two_dim(train_noNA.Image[i]) for i in range(len(train_noNA))])\n",
    "X_noNA = X_noNA / 255.0\n",
    "Y_noNA = np.array([train_noNA.drop('Image', axis = 1).iloc[i] for i in range(len(train_noNA))])\n",
    "\n",
    "\n",
    "shuffle_noNA = np.random.permutation(np.arange(X_noNA.shape[0]))\n",
    "X_noNA, Y_noNA = X_noNA[shuffle_noNA], Y_noNA[shuffle_noNA]\n",
    "train_data_noNA, train_labels_noNA = X_noNA[:1200], Y_noNA[:1200]\n",
    "dev_data_noNA, dev_labels_noNA = X_noNA[1200:], Y_noNA[1200:]\n",
    "numFeatures_noNA = train_data_noNA[1].size\n",
    "numTrainExamples_noNA = train_data_noNA.shape[0]\n",
    "numDevExamples_noNA = dev_data_noNA.shape[0]\n",
    "# numTestExamples = test_data.shape[0]\n",
    "print(f'Train examples {numTrainExamples_noNA}')\n",
    "print(f'Train features {numFeatures_noNA}')\n",
    "print(f'Dev examples {numDevExamples_noNA}')\n",
    "# print(f'Test examples {numTestExamples_noNA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform unlabeled data\n",
    "X_unlab = np.array([two_dim(train_unlab.Image[i]) for i in range(len(train_unlab))])\n",
    "X_unlab = X_unlab / 255.0\n",
    "Y_unlab = np.array([train_unlab.drop('Image', axis = 1).iloc[i] for i in range(len(train_unlab))])\n",
    "\n",
    "\n",
    "shuffle_unlab = np.random.permutation(np.arange(X_unlab.shape[0]))\n",
    "X_unlab, Y_unlab = X_unlab[shuffle_unlab], Y_unlab[shuffle_unlab]\n",
    "train_data_unlab, train_labels_unlab = X_unlab[:3000], Y_unlab[:3000]\n",
    "dev_data_unlab, dev_labels_unlab = X_unlab[3000:], Y_unlab[3000:]\n",
    "numFeatures_unlab = train_data_unlab[1].size\n",
    "numTrainExamples_unlab = train_data_unlab.shape[0]\n",
    "numDevExamples_unlab = dev_data_unlab.shape[0]\n",
    "# numTestExamples = test_data.shape[0]\n",
    "print(f'Train examples {numTrainExamples_unlab}')\n",
    "print(f'Train features {numFeatures_unlab}')\n",
    "print(f'Dev examples {numDevExamples_unlab}')\n",
    "# print(f'Test examples {numTestExamples_unlab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(X, y, n, plot_missing = False):\n",
    "    \"\"\"\n",
    "    plot n images with red dots for labels.\n",
    "    If plot_missing, plot images with missing values (useful for visualizing imputed labels)\"\"\"\n",
    "    if not plot_missing:\n",
    "        for i in range(n):\n",
    "            plt.imshow(X[i],cmap='gray')\n",
    "            #place a point for each of the pictures on the specified coordinates\n",
    "            for loc in range(0, len(y[i]),2):\n",
    "                plt.plot(y[i][loc], y[i][loc+1], '*r')\n",
    "            plt.show()\n",
    "        \n",
    "    elif plot_missing:\n",
    "        for i in train[train['left_eye_outer_corner_y'].isna()].index[:n]:\n",
    "            plt.imshow(X[i],cmap='gray')\n",
    "            #place a point for each of the pictures on the specified coordinates\n",
    "            for loc in range(0, len(y[i]),2):\n",
    "                plt.plot(y[i][loc], y[i][loc+1], '*r')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for showing images\n",
    "def show_images(data, labels, num_examples=3):\n",
    "  \n",
    "  #transform data  into 2D matrix\n",
    "  X2D = np.reshape(data, (-1, 96, 96))\n",
    "\n",
    "  num = num_examples * 3\n",
    "  count = 0\n",
    "\n",
    "  #create a figure \n",
    "  fig, axes = plt.subplots(num_examples, 3, figsize = (9.6, 9.6))\n",
    "\n",
    "\n",
    "  #iterate across the row of images and display one image in each of the num_examples boxes\n",
    "  for n in range(num):\n",
    "      ax = axes[count//num_examples, count%num_examples]\n",
    "      rand = randrange(0, len(data))\n",
    "      ax.imshow(X2D[rand], cmap = 'gray')\n",
    "      count += 1\n",
    "      for loc in range(0, len(labels[n]), 2):\n",
    "          ax.plot(labels[rand][loc], labels[rand][loc+1], '*r')\n",
    "      \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some images with labeled points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#explore some of the images in the training data\n",
    "show_images(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View images without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Same analysis as above but with the noNA dataset - notice the how all the features are marked on the face\n",
    "show_images(train_data_noNA, train_labels_noNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that images with missing values are not as accurate as fully filled records, even for the values it has filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#continue EDA, see distribution of all coordinates in train data except the \"Image\" column\n",
    "train.hist(bins = 30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at removing outliers\n",
    "# train_outliers  = train[((train['left_eye_inner_corner_x'] - train['left_eye_inner_corner_x'].mean()) / train['left_eye_inner_corner_x'].std()).abs() < 3]\n",
    "# train_outliers = train[train.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
    "# train_outliers.hist(bins = 30, figsize=(15,15))\n",
    "# plt.show()\n",
    "\n",
    "#create df for noNA data to do the same analysis as above\n",
    "# tempdf = pd.DataFrame(train_labels_noNA)\n",
    "# tempdf.hist(bins = 30, figsize=(15,15))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next two cells if you want to explore the dev data to make sure it's clean as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tempdf = pd.DataFrame(dev_labels_noNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tempdf.hist(bins = 30, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confirm size of the datasets\n",
    "print(train_data_noNA.shape)\n",
    "print(train_labels_noNA.shape)\n",
    "print(dev_data_noNA.shape)\n",
    "print(dev_labels_noNA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (Keeping Missing Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Fill Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "train_ffill = train.fillna(method = \"ffill\")\n",
    "\n",
    "# transform Y for ffill in same way as non-ffill data (create for ffill imputation visuals)\n",
    "Y_ffill = np.array([train_ffill.drop('Image', axis = 1).iloc[i] for i in range(len(train_ffill))])\n",
    "Y_ffill = Y_ffill[shuffle]\n",
    "\n",
    "# transform train and dev labels for model\n",
    "train_labels_ffill = Y_ffill[:5000]\n",
    "dev_labels_ffill = Y_ffill[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scale data (optimal for KNN distances)\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train.drop(columns = \"Image\"))\n",
    "train_labels_scaled = scaler.fit_transform(train_labels)\n",
    "# mini_train_labels_scaled = scaler.fit_transform(mini_train_labels)\n",
    "dev_labels_scaled = scaler.fit_transform(dev_labels)\n",
    "\n",
    "# KNN Imputation (scaled) (K = 5)\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "train_knn_scaled = imputer.fit_transform(train_scaled)\n",
    "train_labels_knn_scaled = imputer.fit_transform(train_labels_scaled)\n",
    "# mini_train_labels_knn_scaled = imputer.fit_transform(mini_train_labels_scaled)\n",
    "dev_labels_knn_scaled = imputer.fit_transform(dev_labels_scaled)\n",
    "\n",
    "### removing in lieu of scaled KNN\n",
    "# # KNN Imputation (no scale) (K = 5)\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# train_knn = imputer.fit_transform(train.drop(columns=\"Image\"))\n",
    "# train_labels_knn = imputer.fit_transform(train_labels)\n",
    "# # mini_train_labels_knn = imputer.fit_transform(mini_train_labels)\n",
    "# dev_labels_knn = imputer.fit_transform(dev_labels)\n",
    "\n",
    "# Inverse transform after imputation for viewing performance on images\n",
    "train_knn_scaled_inverse = scaler.inverse_transform(train_knn_scaled)\n",
    "train_labels_knn_scaled_inverse = scaler.inverse_transform(train_labels_knn_scaled)\n",
    "# mini_train_labels_knn_scaled_inverse = scaler.inverse_transform(mini_train_labels_knn_scaled)\n",
    "dev_labels_knn_scaled_inverse = scaler.inverse_transform(dev_labels_knn_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm no more missing values (knn)\n",
    "import missingno as msno\n",
    "fig, ax = plt.subplots()\n",
    "msno.bar(pd.DataFrame(train_knn_scaled_inverse, columns = train.drop(columns = \"Image\").columns))\n",
    "ax.set_title(\"Missing Values after KNN\", fontsize = 30, pad = 20)\n",
    "ax.set_xlabel(\"Feature\", fontsize = 25, labelpad = 20)\n",
    "ax.set_ylabel(\"Fill Rate\", fontsize = 25, labelpad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Images of Each Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check original images (for comparison)\n",
    "show_images(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check KNN-plotted images\n",
    "show_images(train_data, train_labels_knn_scaled_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check ffill-plotted images\n",
    "show_images(train_data, Y_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape tuples for reshaping the data for model\n",
    "train_reshape = (5000, 9216)\n",
    "train_noNA_reshape = (1200, 9216)\n",
    "dev_reshape = (2049, 9216)\n",
    "dev_noNA_reshape = (940, 9216)\n",
    "label_reshape= (-1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model with NAs\n",
    "model_base = Sequential()\n",
    "# model.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_base.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_base.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_base.fit(train_data.reshape(train_reshape), train_labels, shuffle=False, batch_size=1, verbose=0, epochs=10) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_base = model_base.evaluate(dev_data.reshape(dev_reshape), dev_labels, verbose=0) \n",
    "print('Test score:', score_base[0]) \n",
    "print('Test accuracy:', score_base[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many NAs in the data, the model is not able to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ffill-imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling on the ffill-imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffill-imputed model\n",
    "model_NN_ff1 = Sequential()\n",
    "# model.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_NN_ff1.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_NN_ff1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_ff1.fit(train_data.reshape(train_reshape), train_labels_ffill, shuffle=False, batch_size=1, verbose=0, epochs=10) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_ff1 = model_NN_ff1.evaluate(dev_data.reshape(dev_reshape), dev_labels_ffill, verbose=0)\n",
    "print('Test score:', score_NN_ff1[0]) \n",
    "print('Test accuracy:', score_NN_ff1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN-imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling on the knn-imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN-imputed model\n",
    "model_NN_knn1 = Sequential()\n",
    "# model.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_NN_knn1.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_NN_knn1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_knn1.fit(train_data.reshape(train_reshape), train_labels_knn_scaled_inverse, shuffle=False, batch_size=1, verbose=0, epochs=10) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_knn1 = model_NN_knn1.evaluate(dev_data.reshape(dev_reshape), dev_labels_knn_scaled_inverse, verbose=0)\n",
    "print('Test score:', score_NN_knn1[0]) \n",
    "print('Test accuracy:', score_NN_knn1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same model but on the cleaned noNA datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Records Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No NA model\n",
    "model_NN_noNA1 = Sequential()\n",
    "# model_NN_noNA1.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model_NN_noNA1.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_NN_noNA1.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_NN_noNA1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_noNA1.fit(train_data_noNA.reshape(train_noNA_reshape), train_labels_noNA, shuffle=False, batch_size=1, verbose=0, epochs=10) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_noNA1 = model_NN_noNA1.evaluate(dev_data_noNA.reshape(dev_noNA_reshape), dev_labels_noNA, verbose=0) \n",
    "print('Test score:', score_NN_noNA1[0]) \n",
    "print('Test accuracy:', score_NN_noNA1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_NN_noNA1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because baseline performance is higher on the missing-value-removed model, and the filled values on the missing-value records are inaccurate, we will proceed with iterating on the no-na model (or maybe we will add an additional label that specifies which records had imputed values) so the model can take that into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try and see if Label prop (Semi-suprivised) can beat the above accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_mix = np.nan_to_num(train_labels, nan = -1) \n",
    "df = pd.DataFrame(train_labels_mix)\n",
    "df = df.round(2).astype(int)\n",
    "\n",
    "df_dev = pd.DataFrame(dev_labels_noNA)\n",
    "df_dev = df_dev.round(2).astype(int)\n",
    "\n",
    "feature_names = list(train.columns)[:30]\n",
    "\n",
    "df_1 = pd.DataFrame()\n",
    "\n",
    "for i in range(30):\n",
    "    model = LabelPropagation()\n",
    "    model.fit(train_data.reshape(train_reshape), df[i])\n",
    "    \n",
    "    new_labels = model.transduction_\n",
    "\n",
    "    df_1[feature_names[i]] = new_labels\n",
    "\n",
    "df_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_train_labels = df_1.to_numpy()\n",
    "lp_train_labels.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_NN_lp = Sequential()\n",
    "# model_NN_lp.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model_NN_lp.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_NN_lp.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_NN_lp.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_lp.fit(train_data.reshape(train_reshape), lp_train_labels, shuffle=False, batch_size=1, verbose=0, epochs=10) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_lp = model_NN_lp.evaluate(dev_data_noNA.reshape(dev_noNA_reshape), dev_labels_noNA, verbose=0) \n",
    "print('Test score:', score_NN_lp[0]) \n",
    "print('Test accuracy:', score_NN_lp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_data, lp_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(30):\n",
    "    model = LabelSpreading()\n",
    "    model.fit(train_data.reshape(train_reshape), df[i])\n",
    "    \n",
    "    new_labels = model.transduction_\n",
    "\n",
    "    df_2[feature_names[i]] = new_labels\n",
    "\n",
    "df_2\n",
    "\n",
    "ls_train_labels = df_2.to_numpy()\n",
    "ls_train_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN_ls = Sequential()\n",
    "# model_NN_ls.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model_NN_ls.add(Dense(30, input_dim=30, activation='sigmoid'))\n",
    "model_NN_ls.add(Dense(units=30, input_dim=9216, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_NN_ls.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_ls.fit(train_data.reshape(train_reshape), ls_train_labels, shuffle=True, batch_size=5, verbose=0, epochs=3) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_ls = model_NN_ls.evaluate(dev_data_noNA.reshape(dev_noNA_reshape), dev_labels_noNA, verbose=0) \n",
    "print('Test score:', score_NN_ls[0]) \n",
    "print('Test accuracy:', score_NN_ls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more layers (Missing Values Removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_NN_noNA2 = Sequential()\n",
    "model_NN_noNA2.add(Dense(30, input_dim=9216, activation='sigmoid'))\n",
    "# model_NN_noNA2.add(Dense(30, input_dim=30, activation='softmax'))\n",
    "model_NN_noNA2.add(Dropout(0.5))\n",
    "model_NN_noNA2.add(Dense(units=30, input_dim=30, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.0001)\n",
    "model_NN_noNA2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model_NN_noNA2.fit(train_data_noNA.reshape(train_noNA_reshape), train_labels_noNA, shuffle=False, batch_size=10, verbose=0, epochs=50) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score_NN_noNA2 = model_NN_noNA2.evaluate(dev_data_noNA.reshape(dev_noNA_reshape), dev_labels_noNA, verbose=0) \n",
    "print('Test score:', score_NN_noNA2[0]) \n",
    "print('Test accuracy:', score_NN_noNA2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No additional gain on original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_NN_noNA2.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Model Baseline Attempt (Missing Values Removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reshape data into 2D arrays\n",
    "x_train_96x96 = train_data_noNA.reshape(train_noNA_reshape).reshape(train_data_noNA.shape[0], 96, 96, 1)\n",
    "x_dev_96x96 = dev_data_noNA.reshape(dev_noNA_reshape).reshape(dev_data_noNA.shape[0], 96, 96, 1)\n",
    "\n",
    "model_CNN_noNA1 = Sequential() \n",
    "model_CNN_noNA1.add(Conv2D(96, kernel_size=(3, 3),activation='relu',input_shape=(96, 96, 1)))\n",
    "model_CNN_noNA1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_CNN_noNA1.add(Conv2D(192, (3, 3), activation='relu'))\n",
    "model_CNN_noNA1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_CNN_noNA1.add(Dropout(0.5))\n",
    "model_CNN_noNA1.add(Flatten())\n",
    "model_CNN_noNA1.add(Dense(units=50, input_dim=384, activation='sigmoid')) \n",
    "model_CNN_noNA1.add(Dense(units=30, input_dim=50, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model_CNN_noNA1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_CNN_noNA1.fit(x_train_96x96, train_labels_noNA, batch_size=100, epochs=5, verbose=1, validation_data=(x_dev_96x96, dev_labels_noNA))\n",
    "score_CNN_noNA1 = model_CNN_noNA1.evaluate(x_dev_96x96, dev_labels_noNA, verbose=0)\n",
    "print('Test score:', score_CNN_noNA1[0]) \n",
    "print('Test accuracy:', score_CNN_noNA1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no improvement in the test accuracy with CNN. No changes with batch size, epochs, or learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_CNN_noNA1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(method='ffill')\n",
    "\n",
    "def load_images(image_data):\n",
    "    images = []\n",
    "    for idx, sample in image_data.iterrows():\n",
    "        image = np.array(sample['Image'].split(' '), dtype=int)\n",
    "        image = np.reshape(image, (96,96,1))\n",
    "        images.append(image)\n",
    "    images = np.array(images)/255.\n",
    "    return images\n",
    "\n",
    "def load_keypoints(keypoint_data):\n",
    "    keypoint_data = keypoint_data.drop(['Image'], axis=1)\n",
    "    keypoint_features = []\n",
    "    for idx, features in keypoint_data.iterrows():\n",
    "        keypoint_features.append(features)\n",
    "    keypoint_features = np.array(keypoint_features, dtype=float)\n",
    "    return keypoint_features\n",
    "\n",
    "train_images  = load_images(train)\n",
    "train_keypoints = load_keypoints(train)\n",
    "test_images = load_images(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_96x96 = train_data_noNA.reshape(train_noNA_reshape).reshape(train_data_noNA.shape[0], 96, 96, 1)\n",
    "x_dev_96x96 = dev_data_noNA.reshape(dev_noNA_reshape).reshape(dev_data_noNA.shape[0], 96, 96, 1)\n",
    "\n",
    "#reshape data into 2D arrays\n",
    "x_train_96x96 = train_data_noNA.reshape(train_noNA_reshape).reshape(train_data_noNA.shape[0], 96, 96, 1)\n",
    "x_dev_96x96 = dev_data_noNA.reshape(dev_noNA_reshape).reshape(dev_data_noNA.shape[0], 96, 96, 1)\n",
    "\n",
    "model_CNN3_noNA_D1 = Sequential() \n",
    "model_CNN3_noNA_D1.add(Conv2D(96, kernel_size=(3, 3),activation='relu',input_shape=(96, 96, 1)))\n",
    "model_CNN3_noNA_D1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_CNN3_noNA_D1.add(Conv2D(192, (3, 3), activation='relu'))\n",
    "model_CNN3_noNA_D1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_CNN3_noNA_D1.add(Conv2D(192, (3, 3), activation='relu')3\n",
    "\n",
    "model_CNN1_noNA_D1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN3_noNA_D1.add(GlobalAveragePooling2D())\n",
    "# model_CNN3_noNA_D1.add(Flatten())\n",
    "model_CNN3_noNA_D1.add(Dropout(0.1))\n",
    "model_CNN3_noNA_D1.add(Dense(30))\n",
    "\n",
    "model_CNN3_noNA_D1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlyStopping = EarlyStopping(monitor='loss', patience=30, mode='min',\n",
    "                             baseline=None)\n",
    "\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-15, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN3_noNA_D1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model_CNN3_noNA_D1.fit(train_images, train_keypoints, batch_size= 64, epochs= 100, validation_split= 0.15, callbacks=[earlyStopping, rlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_CNN_noNA1.predict(dev_data_noNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Pre-trained Resnet and one dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_noNA_D1 = Sequential() \n",
    "pretrained_model = ResNet50(input_shape=(96,96,3), include_top=False, weights='imagenet')\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "model_resnet_noNA_D1.add(Conv2D(3, kernel_size=(1, 1), activation='relu', padding= 'same' ,input_shape=(96, 96, 1)))\n",
    "model_resnet_noNA_D1.add(LeakyReLU(alpha=0.1))\n",
    "model_resnet_noNA_D1.add(pretrained_model)\n",
    "model_resnet_noNA_D1.add(GlobalAveragePooling2D())\n",
    "model_resnet_noNA_D1.add(Dropout(0.1))\n",
    "model_resnet_noNA_D1.add(Dense(30))\n",
    "model_resnet_noNA_D1.summary()\n",
    "\n",
    "model_resnet_noNA_D1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_noNA_D1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model_resnet_noNA_D1.fit(train_images, train_keypoints, batch_size= 64, epochs= 80, validation_split= 0.25, callbacks=[earlyStopping, rlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].set_title('Model Loss', fontsize=12)\n",
    "ax[1].set_title('Model Acc', fontsize=12)\n",
    "fig.suptitle('Model Metrics', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet pretrained with two dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_noNA_D2 = Sequential() \n",
    "pretrained_model = ResNet50(input_shape=(96,96,3), include_top=False, weights='imagenet')\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "model_resnet_noNA_D2.add(Conv2D(3, kernel_size=(1, 1), activation='relu', padding= 'same' ,input_shape=(96, 96, 1)))\n",
    "model_resnet_noNA_D2.add(LeakyReLU(alpha=0.1))\n",
    "model_resnet_noNA_D2.add(Conv2D(3, kernel_size=(1, 1), activation='relu', padding= 'same' ,input_shape=(96, 96, 1)))\n",
    "model_resnet_noNA_D2.add(LeakyReLU(alpha=0.1))\n",
    "model_resnet_noNA_D2.add(pretrained_model)\n",
    "model_resnet_noNA_D2.add(GlobalAveragePooling2D())\n",
    "model_resnet_noNA_D2.add(Dropout(0.2))\n",
    "model_resnet_noNA_D2.add(Dense(96))\n",
    "model_resnet_noNA_D2.add(Dense(30))\n",
    "\n",
    "\n",
    "model_resnet_noNA_D2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_noNA_D2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model_resnet_noNA_D2.fit(train_images, train_keypoints, batch_size= 64, epochs= 80, validation_split= 0.15, callbacks=[earlyStopping, rlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].set_title('Model Loss', fontsize=12)\n",
    "ax[1].set_title('Model Acc', fontsize=12)\n",
    "fig.suptitle('Model Metrics', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnetB7_noNA_D1 = Sequential() \n",
    "pretrained_model = EfficientNetB7(input_shape= (96, 96, 3), include_top= False, weights='imagenet')\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "model_effnetB7_noNA_D1.add(Conv2D(3, kernel_size=(1, 1), activation='relu', padding= 'same' ,input_shape=(96, 96, 1)))\n",
    "model_effnetB7_noNA_D1.add(LeakyReLU(alpha=0.1))\n",
    "model_effnetB7_noNA_D1.add(pretrained_model)\n",
    "model_effnetB7_noNA_D1.add(GlobalAveragePooling2D())\n",
    "model_effnetB7_noNA_D1.add(Dropout(0.1))\n",
    "model_effnetB7_noNA_D1.add(Dense(30))\n",
    "\n",
    "\n",
    "model_effnetB7_noNA_D1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnetB7_noNA_D1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model_effnetB7_noNA_D1.fit(train_images, train_keypoints, batch_size= 64, epochs= 200, validation_split= 0.15, callbacks=[earlyStopping, rlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].set_title('Model Loss', fontsize=12)\n",
    "ax[1].set_title('Model Acc', fontsize=12)\n",
    "fig.suptitle('Model Metrics', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image, label in ds_train.take(1):\n",
    "#     for i in range(9):\n",
    "#         ax = plt.subplot(3, 3, i + 1)\n",
    "#         aug_img = img_augmentation(tf.expand_dims(image, axis=0))\n",
    "#         plt.imshow(aug_img[0].numpy().astype(\"uint8\"))\n",
    "#         plt.title(\"{}\".format(format_label(label)))\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#Great code block for showing augmentation examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet try number 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnetB0_noNA_D1 = Sequential() \n",
    "pretrained_model = EfficientNetB0(input_shape= (96, 96, 3), include_top= False, weights='imagenet')\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "model_effnetB0_noNA_D1.add(Conv2D(3, kernel_size=(1, 1), activation='relu', padding= 'same' ,input_shape=(96, 96, 1)))\n",
    "model_effnetB0_noNA_D1.add(LeakyReLU(alpha=0.1))\n",
    "model_effnetB0_noNA_D1.add(pretrained_model)\n",
    "model_effnetB0_noNA_D1.add(GlobalAveragePooling2D())\n",
    "model_effnetB0_noNA_D1.add(Dropout(0.1))\n",
    "model_effnetB0_noNA_D1.add(Dense(30, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model_effnetB0_noNA_D1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnetB0_noNA_D1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model_effnetB0_noNA_D1.fit(train_images, train_keypoints, batch_size= 64, epochs= 100, validation_split= 0.15, callbacks=[earlyStopping, rlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['loss', 'val_loss']].plot(ax=ax[0])\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
    "ax[0].set_title('Model Loss', fontsize=12)\n",
    "ax[1].set_title('Model Acc', fontsize=12)\n",
    "fig.suptitle('Model Metrics', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adf783bbbd88482478cbb33b573058a9c80ad7e3b8253c9b953b19ddbee4c21"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
